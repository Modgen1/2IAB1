{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming Lesson and Exercises - HYP\n",
    "\n",
    "The goal of these programming lesson and exercises is to teach you how to do hypothesis testing with *Python* and the `StatsModels` library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This notebook has the following structure:\n",
    "\n",
    "- The first part introduces the concepts for this week. The theory is interleaved with small exercises, which have the goal of letting you practice the concepts that were just intruduced.\n",
    "- At the end there are one or more larger exercises, which have the goal to test what you have learned earlier. These will be more difficult and will require more independent work than the exercises in the first part.\n",
    "\n",
    "All exercises can be solved with the concepts that were introduced earlier. Since there are often more than one correct way to solve a programming problem, we try to accept various correct anwers. However, many of the automatic tests in Momotor (see _How to submit your work_ below) assume that your answers are constructed using the concepts introduced in these notebooks. If you look for answers on the Internet (e.g. if you import other libraries) you run the risk that your answers will be rejected.\n",
    "\n",
    "Some of the small exercises can be solved by copy-pasting code from the examples. However, it is up to you to try to solve the exercises yourself, which will help you learn, before copy-pasting the answers. The ease of looking up answers is meant to provide guidance when you get stuck, especially for those of you who are new to programming.\n",
    "\n",
    "For your convenience, in the `support` directory you will find a summary of the Python methods introduced in this notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Introduction to This Template Notebook\n",
    "\n",
    "* This is a **personal** notebook.\n",
    "* Make sure you work in a **copy** of `...-template.ipynb`,\n",
    "**renamed** to `...-yourIDnr.ipynb`,\n",
    "where `yourIDnr` is your TU/e identification number.\n",
    "\n",
    "<div class=\"alert alert-danger\" role=\"danger\">\n",
    "<h3>Integrity</h3>\n",
    "<ul>\n",
    "    <li>In this course you must act according to the rules of the TU/e code of scientific conduct.</li>\n",
    "    <li>All the exercises and the graded assignments are to be executed individually and independently.</li>\n",
    "    <li>You must not copy from the Internet, your friends, books... If you represent other people's work as your own, then that constitutes fraud and will be reported to the Examination Committee.</li>\n",
    "    <li>Making your work available to others (complicity) also constitutes fraud.</li>\n",
    "</ul>\n",
    "</div>\n",
    "\n",
    "You are expected to work with Python code in this notebook.\n",
    "\n",
    "The locations where you should write your solutions can be recognized by\n",
    "**marker lines**,\n",
    "which look like this:\n",
    "\n",
    ">`#//`\n",
    ">    `BEGIN_TODO [Label]` `Description` `(n points)`\n",
    ">\n",
    ">`#//`\n",
    ">    `END_TODO [Label]`\n",
    "\n",
    "<div class=\"alert alert-warning\" role=\"alert\">Do NOT modify or delete these marker lines.  Keep them as they are.<br/>\n",
    "NEVER write code <i>outside</i> the marked blocks.\n",
    "Such code cannot be evaluated.\n",
    "</div>\n",
    "\n",
    "Proceed in this notebook as follows:\n",
    "* **Read** the text.\n",
    "* **Fill in** your solutions between `BEGIN_TODO` and `END_TODO` marker lines.\n",
    "* **Run** _all_ code cells (also the ones _without_ your code),\n",
    "    _in linear order_ from the first code cell.\n",
    "\n",
    "**Personalize your notebook**:\n",
    "1. Copy the following three lines of code:\n",
    "\n",
    "  ```python\n",
    "  AUTHOR_NAME = 'Your Full Name'\n",
    "  AUTHOR_ID_NR = '1234567'\n",
    "  AUTHOR_DATE = 'YYYY-MM-DD'\n",
    "  ```\n",
    "1. Paste them between the marker lines in the next code cell.\n",
    "1. Fill in your _full name_, _identification number_, and the current _date_ (i.e. when you first modified this notebook, e.g. '2023-11-01') as strings between the `Author` markers.\n",
    "1. Run the code cell by putting the cursor there and typing **Control-Enter**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO [Author] Name, Id.nr., Date, as strings (1 point)\n",
    "\n",
    "AUTHOR_NAME = 'Ivan Sergeevich Mishin'\n",
    "AUTHOR_ID_NR = '2076209'\n",
    "AUTHOR_DATE = '2024-12-30'\n",
    "\n",
    "#// END_TODO [Author]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "- [Introduction](#Introduction)\n",
    "    - [Concepts and Terminology](#Concepts-and-Terminology)\n",
    "    - [Steps to Address the Problem](#Steps-to-Address-the-Problem)\n",
    "- [Loading the Libraries](#Loading-the-Libraries)\n",
    "- [1. Hypothesis Testing for Equality of Means](#1.-Hypothesis-Testing-for-Equality-of-Means)\n",
    "    - [Learning Objectives of This Section](#Learning-Objectives-of-This-Section)\n",
    "    - [Example: Speed of Light](#Example:-Speed-of-Light)\n",
    "    - [Step 1 (Speed of Light): Define Quantitative Question](#Step-1-(Speed-of-Light):-Define-Quantitative-Question)\n",
    "    - [Step 2 (Speed of Light): Formulate Hypothesis](#Step-2-(Speed-of-Light):-Formulate-Hypothesis)\n",
    "    - [Step 3 (Speed of Light): Check Assumptions](#Step-3-(Speed-of-Light):-Check-Assumptions)\n",
    "        - [Kernel Density Plot](#Kernel-Density-Plot)\n",
    "        - [Q-Q Plot](#Q-Q-Plot)\n",
    "        - [Anderson-Darling Normality Test](#Anderson-Darling-Normality-Test)\n",
    "    - [Step 4 (Speed of Light): Apply Appropriate Test](#Step-4-(Speed-of-Light):-Apply-Appropriate-Test)\n",
    "        - [t-test for Equality of Means (One-sample)](#t-test-for-Equality-of-Means-(One-sample))\n",
    "        - [t-test for Equality of Means (Two-sample)](#t-test-for-Equality-of-Means-(Two-sample))\n",
    "    - [Step 5 (Speed of Light): Decision](#Step-5-(Speed-of-Light):-Decision)\n",
    "        - [Confidence Interval](#Confidence-Interval)\n",
    "- [Exercise: Cloud Seeding](#Exercise:-Cloud-Seeding)\n",
    "    - [Step 1 (Cloud Seeding): Define Quantitative Question](#Step-1-(Cloud-Seeding):-Define-Quantitative-Question)\n",
    "        - [Exercise 1.a](#Exercise-1.a)\n",
    "    - [Step 2 (Cloud Seeding): Formulate Hypothesis](#Step-2-(Cloud-Seeding):-Formulate-Hypothesis)\n",
    "        - [Exercise 1.b](#Exercise-1.b)\n",
    "    - [Step 3 (Cloud Seeding): Check Assumptions](#Step-3-(Cloud-Seeding):-Check-Assumptions)\n",
    "        - [Exercise 1.c](#Exercise-1.c)\n",
    "        - [Exercise 1.d](#Exercise-1.d)\n",
    "        - [Exercise 1.e](#Exercise-1.e)\n",
    "        - [Exercise 1.f](#Exercise-1.f)\n",
    "        - [Exercise 1.g](#Exercise-1.g)\n",
    "        - [Exercise 1.h](#Exercise-1.h)\n",
    "        - [Exercise 1.i](#Exercise-1.i)\n",
    "    - [Step 4 (Cloud Seeding): Apply Appropriate Test](#Step-4-(Cloud-Seeding):-Apply-Appropriate-Test)\n",
    "        - [Exercise 1.j](#Exercise-1.j)\n",
    "        - [Exercise 1.k](#Exercise-1.k)\n",
    "        - [Exercise 1.l](#Exercise-1.l)\n",
    "    - [Step 5 (Cloud Seeding): Decision](#Step-5-(Cloud-Seeding):-Decision)\n",
    "        - [Exercise 1.m](#Exercise-1.m)\n",
    "- [2. Hypothesis Testing on Proportions](#2.-Hypothesis-Testing-on-Proportions)\n",
    "    - [Learning Objectives of This Section](#Learning-Objectives-of-This-Section)\n",
    "    - [Example: Exams](#Example:-Exams)\n",
    "    - [Step 1 (Exams): Define Quantitative Question](#Step-1-(Exams):-Define-Quantitative-Question)\n",
    "    - [Step 2 (Exams): Formulate Hypothesis](#Step-2-(Exams):-Formulate-Hypothesis)\n",
    "    - [Step 3 (Exams): Check Assumptions](#Step-3-(Exams):-Check-Assumptions)\n",
    "    - [Step 4 (Exams): Apply Appropriate Test](#Step-4-(Exams):-Apply-Appropriate-Test)\n",
    "        - [z-test for Equality of Proportions (One-sample)](#z-test-for-Equality-of-Proportions-(One-sample))\n",
    "        - [z-test for Equality of Proportions (Two-sample)](#z-test-for-Equality-of-Proportions-(Two-sample))\n",
    "        - [Extract the p-value](#Extract-the-p-value)\n",
    "        - [Extract the Confidence Interval (One-sample)](#Extract-the-Confidence-Interval-(One-sample))\n",
    "        - [Extract the Confidence Interval (Two-sample)](#Extract-the-Confidence-Interval-(Two-sample))\n",
    "    - [Step 5 (Exams): Decision](#Step-5-(Exams):-Decision)\n",
    "        - [p-value](#p-value)\n",
    "        - [Confidence Interval](#Confidence-Interval)\n",
    "- [Exercise: Silicon Wafers](#Exercise:-Silicon-Wafers)\n",
    "    - [Exercise 2.a](#Exercise-2.a)\n",
    "    - [Step 1 (Silicon Wafers): Define a Quantitative Question](#Step-1-(Silicon-Wafers):-Define-a-Quantitative-Question)\n",
    "        - [Exercise 2.b](#Exercise-2.b)\n",
    "    - [Step 2 (Silicon Wafers): Formulate Hypothesis](#Step-2-(Silicon-Wafers):-Formulate-Hypothesis)\n",
    "        - [Exercise 2.c](#Exercise-2.c)\n",
    "    - [Step 3 (Silicon Wafers): Check Assumptions](#Step-3-(Silicon-Wafers):-Check-Assumptions)\n",
    "        - [Exercise 2.d](#Exercise-2.d)\n",
    "    - [Step 4 (Silicon Wafers): Apply Correct Test](#Step-4-(Silicon-Wafers):-Apply-Correct-Test)\n",
    "        - [Exercise 2.e](#Exercise-2.e)\n",
    "        - [Exercise 2.f](#Exercise-2.f)\n",
    "- [3. Performing Diagnostics on Regression Results](#3.-Performing-Diagnostics-on-Regression-Results)\n",
    "    - [Learning Objectives of This Section](#Learning-Objectives-of-This-Section)\n",
    "    - [Example: Boiling Point of Water](#Example:-Boiling-Point-of-Water)\n",
    "        - [Model Evaluation](#Model-Evaluation)\n",
    "    - [Exercise: Timber](#Exercise:-Timber)\n",
    "        - [Exercise 3.a](#Exercise-3.a)\n",
    "        - [Exercise 3.b](#Exercise-3.b)\n",
    "        - [Exercise 3.c](#Exercise-3.c)\n",
    "    - [First Order Polynomial Model](#First-Order-Polynomial-Model)\n",
    "        - [Exercise 3.d](#Exercise-3.d)\n",
    "        - [Exercise 3.e](#Exercise-3.e)\n",
    "        - [Exercise 3.f](#Exercise-3.f)\n",
    "        - [Exercise 3.g](#Exercise-3.g)\n",
    "        - [Exercise 3.h](#Exercise-3.h)\n",
    "    - [Better Model](#Better-Model)\n",
    "        - [Exercise 3.i](#Exercise-3.i)\n",
    "        - [Exercise 3.j](#Exercise-3.j)\n",
    "        - [Exercise 3.k](#Exercise-3.k)\n",
    "        - [Exercise 3.l](#Exercise-3.l)\n",
    "        - [Exercise 3.m](#Exercise-3.m)\n",
    "- [4. Exercise: Hypothesis Testing](#4.-Exercise:-Hypothesis-Testing)\n",
    "    - [Exercise 4.a](#Exercise-4.a)\n",
    "    - [Exercise 4.b](#Exercise-4.b)\n",
    "    - [Exercise 4.c](#Exercise-4.c)\n",
    "    - [Exercise 4.d](#Exercise-4.d)\n",
    "    - [Exercise 4.e](#Exercise-4.e)\n",
    "    - [Exercise 4.f](#Exercise-4.f)\n",
    "- [5. Exercise: Regression Analysis](#5.-Exercise:-Regression-Analysis)\n",
    "    - [Exercise 5.a](#Exercise-5.a)\n",
    "    - [Exercise 5.b](#Exercise-5.b)\n",
    "    - [Exercise 5.c](#Exercise-5.c)\n",
    "    - [Exercise 5.d](#Exercise-5.d)\n",
    "    - [Exercise 5.e](#Exercise-5.e)\n",
    "    - [Exercise 5.f](#Exercise-5.f)\n",
    "    - [Exercise 5.g](#Exercise-5.g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this lesson,\n",
    "we explain how to do\n",
    "hypothesis testing with Python and _StatsModels_ for (all combinations of)\n",
    "\n",
    "* _one-sided_ and _two-sided_ tests,\n",
    "* with _one-sample_ and _two-sample_ data,\n",
    "* for _equality of means_ and for _equality of proportions_.\n",
    "\n",
    "We also demonstrate how to check the underlying normality assumption on the data.  \n",
    "In addition, we explain how to perform diagnostics on a regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concepts and Terminology\n",
    "\n",
    "The following concepts are involved in a statistical hypothesis test:\n",
    "    \n",
    "1. **Null hypothesis $H_0$**\n",
    "    (the \"there-is-nothing-special-to-this-and-it-is-all-just-coincidence\" assumption) and\n",
    "    **alternative hypothesis $H_a$** (the \"there-is-something-special-to-this\" assumption).\n",
    "\n",
    "2. **Level of significance**, also known as $\\alpha$-value.\n",
    "    It is the probability of a _Type-I error_ that one is willing to accept,\n",
    "    that is, the probability that the null hypothesis will be rejected while it is actually true.\n",
    "    Typically $\\alpha = 0.05$.\n",
    "\n",
    "3. **Test statistic**:\n",
    "    a value calculated from the observed data according to a standardized formula,\n",
    "    which is used in the hypothesis test (function) to either reject or fail to reject $H_0$.\n",
    "    It depends on the type of test being applied.\n",
    "\n",
    "4. **p-value** (probability of the observed data under the assumption of the null hypothesis)\n",
    "    and/or **confidence interval**\n",
    "    (an interval estimated from the observed data that contains the actual value with a given\n",
    "    probability, the level of confidence, which is typically $1 - \\alpha$).\n",
    "    \n",
    "The decision rule is as follows:\n",
    "if, under the assumption that the null hypothesis holds, the probability of the observed data\n",
    "is less than $\\alpha$,\n",
    "then the null hypothesis is _rejected_;\n",
    "that is, the observed data is considered statistically significant ('special')\n",
    "under the null hypothesis;\n",
    "that is, the observed data is not likely to be explained by coincidence under the null hypothesis.\n",
    "\n",
    "Equivalently, this is the case if and only if the computed p-value is less than the significance level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps to Address the Problem\n",
    "\n",
    "In the lecture you were shown how to statistically address scientific problems. We will now formalize 5 steps to address these problems:\n",
    "1. Define quantitative question.\n",
    "2. Formulate hypothesis.\n",
    "3. Pick correct test and test if its assumptions are met.\n",
    "4. Apply correct test.\n",
    "5. Decision on hypothesis.\n",
    "\n",
    "These steps will be further explained in the rest of this exercise set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Libraries\n",
    "\n",
    "To show examples, we load some Data Analytics libraries first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns                               # also improves the look of plots\n",
    "import warnings                                     # [EV]: Added to filter out some warnings\n",
    "\n",
    "# Configure plots\n",
    "sns.set()                                           # set Seaborn defaults\n",
    "plt.rcParams['figure.figsize'] = 10, 5              # default hor./vert. size of plots, in inches\n",
    "\n",
    "import math\n",
    "from sklearn.linear_model import LinearRegression   # for linear regression\n",
    "\n",
    "# Reveal a hint only while holding the mouse down\n",
    "from IPython.display import HTML\n",
    "HTML('<style>.h,.c{display:none}.t{color:#296eaa}.t:active+.h{display:block;}</style>')\n",
    "\n",
    "# [EV]: Filtering out some warnings\n",
    "warnings.filterwarnings(\"ignore\", \"use_inf_as_na\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we load the _StatsModels_ library for doing statistics in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import statsmodels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Hypothesis Testing for Equality of Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Objectives of This Section\n",
    "\n",
    "After this section, you should\n",
    "\n",
    "* know how to do _one-sided_ and _two-sided_ tests for **equality of the means**\n",
    "    of _one-sample_ and _two-sample_ data;\n",
    "  \n",
    "* know how to test whether data follows a normal distribution using:\n",
    "  * a density plot;\n",
    "  * a Q-Q plot;\n",
    "  * the Anderson-Darling test statistic;\n",
    "  \n",
    "* know how to draw conclusions about acceptance or rejection of the null hypothesis based on:\n",
    "  * p-value resulting from the t-test;\n",
    "  * confidence intervals.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Speed of Light\n",
    "\n",
    "In the 19th century,\n",
    "French physicists Fizeau and Foucault independently invented ways to measure the speed of light in vacuum.\n",
    "Foucault's method turned out to be the most accurate one.\n",
    "The Foucault method is based on fast rotation mirrors.\n",
    "The American physicists Michelson and Newcomb improved Foucault's method. \n",
    "The dataset `datasets/light_michelson.csv` contains measurements by Michelson from 1880 and `datasets/light_newcomb.csv` contains measurements by Newcomb from 1882."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_michelson = pd.read_csv(\"datasets/light_michelson.csv\")\n",
    "df_michelson.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_newcomb = pd.read_csv(\"datasets/light_newcomb.csv\")\n",
    "df_newcomb.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 (Speed of Light): Define Quantitative Question\n",
    "\n",
    "We wish to test whether Newcombs measurements where accurate. The speed of light in vacuum is defined to be 299 792.458 km/s. We can define a quantitative question as follows:\n",
    "\n",
    "> Is Newcomb's estimate of the speed of light significantly different from 299 792.458 km/s?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 (Speed of Light): Formulate Hypothesis\n",
    "\n",
    "As we have seen in the lectures, there are two different types of hypotheses which each have their own associated hypothesis test to test them. We are either interested in the **equality of means** or in the **equality of proportions**. It is important to consciously choose between the two.\n",
    "\n",
    "In our case we are interested in the speed of light measured by Newcomb. We want to compare the experimental data mean (sample average) with the currently known value. Hence we are interested in the **equality of means** on **one-sample** data.\n",
    "\n",
    "As defined in step 1, we are interested in equality. We do not hypothesize Newcombs measurement to be greater or smaller. Therefore we should formulate a **two-sided** alternative hypothesis. \n",
    "\n",
    "In short, we will define a **two-sided** hypothesis for **equality of means** on **one-sample** data.  \n",
    "The corresponding hypotheses are\n",
    "\n",
    "$$H_0:\\mu_{newcomb}=299792.458$$\n",
    "\n",
    "$$H_a:\\mu_{newcomb}\\neq299792.458$$ \n",
    "\n",
    "The appropriate test you were taught for this situation is the **t-test for equality of means**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 (Speed of Light): Check Assumptions\n",
    "\n",
    "As explained in the lecture, a **t-test for equality of means** is appropriate under the assumption that the data comes from a normal distribution *or* the dataset is large. If the dataset is small *and* not normally distributed, the test would not be valid. Therefore, before performing the test we should verify that this assumption holds.\n",
    "\n",
    "As a rule of thumb in these exercises, a dataset is considered small if it contains fewer than 50 samples. In general, deciding if a dataset should be considered small is complicated, and outside the scope of this course.\n",
    "\n",
    "We present three techniques to test for normality:\n",
    "\n",
    "* a **kernel density plot** (which we already encountered in the EDA exercises);\n",
    "* a **Q-Q plot** (which was mentioned in the EDA Lecture, but not yet practiced);\n",
    "* the **Anderson-Darling normality test**.\n",
    "\n",
    "Note that when the dataset is large, normality is not needed for the t-test for equality of means to be valid.\n",
    "\n",
    "In the examples below we will test the `df_newcomb` data for normality. Let's see how large it is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_newcomb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this dataset contains more than 50 samples, strictly speaking we would not need to test for normality before performing the t-test for equality of means. However, we will still do so to illustrate the methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kernel Density Plot\n",
    "\n",
    "This is a visual check, where you compare the kernel density plot of the actual data to the\n",
    "'ideal' bell curve of the normal (or Gaussian) distribution.\n",
    "\n",
    "Let's first set the default figure size to a square aspect ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = 8, 8  # square plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the kernel density plot for Newcomb's measurements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_newcomb.plot(kind='density')\n",
    "plt.xlabel(\"Speed [km/s]\")\n",
    "plt.title(\"KDE plot of the Newcomb measurements\", size=16, weight='bold');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite a small wobble, the distribution mostly resembles a normal distribution.\n",
    "\n",
    "#### Q-Q Plot\n",
    "\n",
    "While a kernel density plot gives a global overview and can easily indicate patterns in the data like two groups or asymmetry, you cannot see whether the normal distribution really fits (e.g., you cannot easily ascertain from it whether tails decay sufficiently fast). This is where a Q-Q plot can be used.\n",
    "\n",
    "In a Q-Q plot, the data is transformed such that a normal distribution would follow a straight line at 45 degrees. The straight line is convenient since it is easy to check visually whether points are on a straight line.\n",
    "\n",
    "You can create Q-Q plots using **`sm.qqplot()`**\n",
    "([documentation](https://www.statsmodels.org/stable/generated/statsmodels.graphics.gofplots.qqplot.html)).\n",
    "\n",
    "> **Note:** Unlike the other plotting methods we used in this course, `sm.qqplot()` does not return an `axes` object. Therefore, to assign a title using `.set_title()` we manually create a 1x1 subplot.\n",
    "\n",
    "Let us create a Q-Q plot for the speed of light data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=1, sharex=True, sharey=True, squeeze=False)\n",
    "sm.qqplot(df_newcomb['Speed [km/s]'], fit=True, line='45', ax=ax[0,0])\n",
    "ax[0,0].set_title('Q-Q plot for Newcombs measurements');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a Q-Q plot, the closer the data points stay to the diagonal (45-degree line in red),\n",
    "the more the data follows a normal distribution.  \n",
    "Here, the data again seems to be reasonably normally distributed.\n",
    "\n",
    "#### Anderson-Darling Normality Test\n",
    "\n",
    "We can also compute a (numerical) test statistic that tests for normality,\n",
    "using **`sm.stats.normal_ad()`**\n",
    "([documentation](https://www.statsmodels.org/stable/generated/statsmodels.stats.diagnostic.normal_ad.html)).\n",
    "It returns a pair: the **Anderson-Darling test statistic** (a floating-point number) and\n",
    "its **p-value**.\n",
    "\n",
    "When conducting an Anderson Darling test, we assume that the data is normally distributed. This is our null hypothesis. The first number returned by the function is the test statistic. The second number is the p-value. This p-value expresses the likelihood that the test statistic has this value under the null hypothesis. If this p-value is lower than 0.05, there is a less than 5% chance that this data would occur when drawn from a normal distribution. In that case we reject the null hypothesis and thus normality of our data.\n",
    "\n",
    "We compute the Anderson-Darling test statistic and the p-value for the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.stats.normal_ad(df_newcomb['Speed [km/s]'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our case,\n",
    "the p-value (the **second** element of the tuple) is much larger than 0.05,\n",
    "indicating that this data set is **not** unlikely under the assumption\n",
    "that it is from a normal distribution.\n",
    "\n",
    "None of the three methods provide evidence that the normality assumption is violated, so we can safely apply the **t-test for equality of means**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 (Speed of Light): Apply Appropriate Test\n",
    "\n",
    "We defined a **two-sided** hypothesis for **equality of means** on **one-sample** data and verified that the assumption of normality does not appear to be violated. Now we can apply the appropriate test. In our case this is the **t-test for equality of means**.\n",
    "\n",
    "We wish to test whether the mean of the sample is equal to the currently known value for the speed of light, or if there is a significant difference.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### t-test for Equality of Means (One-sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience, we repeat our hypothesis\n",
    "\n",
    "$$H_0:\\mu_\\text{newcomb} = 299792.458$$\n",
    "\n",
    "$$H_a:\\mu_\\text{newcomb} \\neq 299792.458$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carrying out this kind of test in Python with _StatsModels_ involves three steps:\n",
    "\n",
    "1. **Preprocess the data** using **ds = `sm.stats.DescrStatsW()`**.\n",
    "1. **Extract test parameters** using\n",
    "\n",
    "    * **`ds.ttest_mean(value, alternative='...')`** for **t-test statistic**, **p-value**, and\n",
    "        **degrees of freedom**,\n",
    "    * **`ds.tconfint_mean(alpha=..., alternative='...')`** for the **confidence interval**\n",
    "    \n",
    "    where\n",
    "    \n",
    "    * `value` is the value we are comparing the sample mean to,\n",
    "    * `alternative=` one of \n",
    "\n",
    "        - `'two-sided'` (meaning the alternative hypothesis is $\\mu \\neq \\mu_0$);\n",
    "        - `'larger'` (meaning the alternative hypothesis is $\\mu > \\mu_0$),\n",
    "        - `'smaller'` (meaning the alternative hypothesis is $\\mu < \\mu_0$),\n",
    "\n",
    "    * `alpha=` the significance level, e.g. `alpha=0.05` corresponds to a 95% confidence interval."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocess the data (one-sample)**\n",
    "\n",
    "First, we preprocess the data sample and its expected mean separately.\n",
    "The resulting objects will be inputs to the actual comparison test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_newcomb = sm.stats.DescrStatsW(df_newcomb['Speed [km/s]'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extract t-test statistic and p-value (one-sample)**\n",
    "\n",
    "The function `ds.ttest_mean(alternative='...')`\n",
    "returns the value of the t-test statistic and its p-value.\n",
    "It also returns as a third value the *degrees of freedom*,\n",
    "but this value can be ignored since we will not use it.\n",
    "\n",
    "Here are these values for the two-sided test, as we are considering the following alternative hypothesis:\n",
    "$𝐻_𝑎:\\mu_{newcomb}\\neq 299792.458$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_newcomb.ttest_mean(299792.458, alternative='two-sided')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **p-value** is the value in the middle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extract confidence interval (one-sample)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `ds.tconfint_mean(alpha=..., alternative='...')` returns the\n",
    "confidence interval for the mean of the sample, with a confidence level of $1 - \\alpha$.\n",
    "For $\\alpha=0.05$ this interval contains the mean with a probability of 0.95."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_newcomb.tconfint_mean(alpha=0.05, alternative='two-sided')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note:** A one-sided interval with $\\alpha=0.025$ has one boundary in common with the two-sided interval with $\\alpha=0.05$. See the output of the next code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_newcomb.tconfint_mean(alpha=0.025, alternative='larger'), d_newcomb.tconfint_mean(alpha=0.025, alternative='smaller')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### t-test for Equality of Means (Two-sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now compare the measurements by Newcomb to those by Michelson to see if their speed of light estimates were actually that different.  \n",
    "Our hypothesis becomes\n",
    "\n",
    "$$H_0:\\mu_\\text{newcomb} = \\mu_\\text{michelson}$$\n",
    "\n",
    "$$H_a:\\mu_\\text{newcomb} \\neq \\mu_\\text{michelson}$$\n",
    "\n",
    "This is an example of a **two-sided** hypothesis for **equality of means** on **two-sample** data.\n",
    "\n",
    "> **Note:** Before we apply the test we must check that all the assumptions are met, i.e. that both samples (Newcomb's and Michelson's) come from a normal distribution or that they are large. We did the check for Newcomb data already. If you do the check for Michelson data you will see that the condtion is also met."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carrying out this kind of test in Python with _StatsModels_ involves two steps:\n",
    "\n",
    "1. **Preprocess the data** using **`sm.stats.DescrStatsW()`**, creating *descriptive statistics objects* **`d1`** and **`d2`** for the two samples we want to compare,\n",
    "1. **Extract test parameters** using\n",
    "    * **`cm = sm.stats.CompareMeans(d1, d2)`** to create a *test object* **`cm`**, comparing **`d1`** to **`d2`**,\n",
    "    * **`cm.ttest_ind(alternative='...')`** for **t-test statistic**, **p-value**, and\n",
    "        **degrees of freedom**,\n",
    "    * **`cm.tconfint_diff(alpha=..., alternative='...')`** for the **confidence interval**,\n",
    "    \n",
    "    where\n",
    "    * `alternative=` one of\n",
    "        - `'two-sided'` (meaning the alternative hypothesis is $\\mu_1 \\neq \\mu_2$),\n",
    "        - `'larger'` (meaning the alternative hypothesis is $\\mu_1 > \\mu_2$),\n",
    "        - `'smaller'` (meaning the alternative hypothesis is $\\mu_1 < \\mu_2$),\n",
    "    * `alpha=` the significance level, e.g. `alpha=0.05` corresponding to the 95% confidence interval."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocess the data (two-sample)**\n",
    "\n",
    "First, we preprocess the data sample and its expected mean separately.\n",
    "The resulting objects will be inputs to the actual comparison test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_newcomb = sm.stats.DescrStatsW(df_newcomb['Speed [km/s]'])\n",
    "d_michelson = sm.stats.DescrStatsW(df_michelson['Speed [km/s]'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create the test object using the preprocessed data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = sm.stats.CompareMeans(d_newcomb, d_michelson)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extract t-test statistic and p-value (two-sample)**\n",
    "\n",
    "The function `cm.ttest_ind(alternative='...')`\n",
    "returns the value of the t-test statistic and its p-value.\n",
    "It also returns as third value the *degrees of freedom*,\n",
    "but this value can be ignored since we will not use it.\n",
    "\n",
    "Here are these values for the two-sided test, as we are considering the following alternative hypothesis:\n",
    "$𝐻_𝑎:\\mu_{newcomb} \\neq \\mu_{michelson}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm.ttest_ind(alternative='two-sided')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **p-value** is the value in the middle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extract confidence interval (two-sample)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `cm.tconfint_diff(alpha=..., alternative='...')` returns the\n",
    "confidence interval for the _difference between the means_ of the two samples (first minus second),\n",
    "with a confidence level of $1 - \\alpha$.\n",
    "For $\\alpha=0.05$ this interval contains the actual difference between the means with a probability of 0.95."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm.tconfint_diff(alpha=0.05, alternative='two-sided')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5 (Speed of Light): Decision\n",
    "\n",
    "The **p-value** and the **confidence interval** both express the same information in a different form. \n",
    "\n",
    "#### p-value\n",
    "\n",
    "The **p-value** is the easiest to interpret. When it is lower than a chosen significance level $\\alpha$ (typically $0.05$) we reject the null hypothesis.\n",
    "- From the p-value calculated in step 4 for the *one-sample* case, \n",
    "we see that in the case of a two-sided test with $\\alpha=0.05$,\n",
    "the *null hypothesis is rejected* (as this p-value is smaller than $0.05$).\n",
    "\n",
    "- From the p-value calculated in step 4 for the *two-sample* case, \n",
    "we see that in the case of a two-sided test with $\\alpha=0.05$,\n",
    "the *null hypothesis is also rejected*.\n",
    "\n",
    "#### Confidence Interval\n",
    "\n",
    "For the *one-sample* case, the **confidence interval** can be interpreted in the following way. The confidence interval expresses the values between which the *mean* lies with 95% confidence. If the confidence interval contains the reference value (i.e., no evidence for a difference in means) we cannot reject the null hypothesis. Otherwise, we reject the null hypothesis.\n",
    "\n",
    "From the confidence interval calculated in step 4 for the *one-sample* case, we can see that the *null hypothesis is rejected*, because it does not contain the reference value for speed of light 299792.458.\n",
    "\n",
    "For the *two-sample* case, the **confidence interval** can be interpreted in the following way. The confidence interval expresses the values between which the *difference between means* lies with 95% confidence. If the confidence interval contains 0 (i.e., no evidence for a difference in means) we cannot reject the null hypothesis. Otherwise, we reject the null hypothesis.\n",
    "\n",
    "From the confidence interval calculated in step 4 for the *two-sample* case, we can see that the *null hypothesis is rejected*, because it does not contain the value 0, meaning that means are different.\n",
    "\n",
    "Observe that the results of the hypothesis tests according to the p-values and the confidence intervals are the same, which is as it should be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Cloud Seeding\n",
    "\n",
    "You are now going to repeat the preceding tests for equality of the means for a different dataset on _cloud seeding_.\n",
    "\n",
    "To improve rain fall in dry areas, an experiment was carried out with 52 clouds.\n",
    "Scientists investigated whether the addition of _silver nitrate_ __increases__ the rainfall.\n",
    "They chose 26 out of a sample of 52 clouds and seeded them with silver nitrate.\n",
    "The remaining 26 clouds were not treated with silver nitrate.\n",
    "\n",
    "The data set `./datasets/clouds.csv` records the rainfall in _feet per acre_.\n",
    "The dataset was constructed to test whether the addition of silver nitrate on clouds causes more rainfall.\n",
    "There are two samples: one column _with_ the silver nitrate treatment and one column _without_ the treatment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clouds = pd.read_csv(\"./datasets/clouds.csv\")\n",
    "df_clouds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task in this exercise is to perform the correct statistical test to investigate whether cloud seeding has the desired effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 (Cloud Seeding): Define Quantitative Question\n",
    "\n",
    "#### Exercise 1.a\n",
    "Choose the option that matches our situation best and assign either `'a'`, `'b'`, `'c'` or `'d'` to the variable `question_cloud`.\n",
    "\n",
    "> a. Does cloud seeding work? <br>\n",
    "> b. Is there a difference in average rainfall when cloud seeding has been applied? <br>\n",
    "> c. Does the application of cloud seeding increase average rainfall?<br>\n",
    "> d. Does the application of cloud seeding increase proportion of rainfall?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO [HYP_1a] Define quantitative question (1 point)\n",
    "\n",
    "# ===== =====> Replace this line by your code. <===== ===== #\n",
    "\n",
    "#// END_TODO [HYP_1a]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 (Cloud Seeding): Formulate Hypothesis\n",
    "\n",
    "#### Exercise 1.b\n",
    "Formulate statistical hypotheses based on the question you chose above. Encode them as strings in the following way:\n",
    "\n",
    "- For the null hypothesis use `'mu_with=mu_without'`, using `mu_with` and `mu_without` to denote the means of the clouds with and without cloud seeding respectively.\n",
    "- For the alternative hypothesis use `!=`, `>` or `<`, e.g. `'mu_with<mu_without'`.\n",
    "\n",
    "Assign the null and alternative hypotheses encoded as strings to variables `H_0_cloud` and `H_a_cloud`, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO [HYP_1b] Formulate hypothesis(1 point)\n",
    "\n",
    "# ===== =====> Replace this line by your code. <===== ===== #\n",
    "\n",
    "#// END_TODO [HYP_1b]\n",
    "H_0_cloud, H_a_cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 (Cloud Seeding): Check Assumptions\n",
    "\n",
    "The t-test is valid for large datasets, or for small datasets that follow the normal distribution. Our rule of thumb in these exercises says that a dataset is considered small when it contains fewer than 50 samples. If we look at the number of samples in the `df_clouds` dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_clouds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can see that there are relatively few samples, so we should check for normality. We will use the three techniques to test for normality:\n",
    "\n",
    "* A **kernel density plot**\n",
    "* A **Q-Q plot** \n",
    "* The **Anderson-Darling normality test**\n",
    "\n",
    "Note that if the dataset is large, we do not need to check for normality before applying the t-test for equality of means."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1.c\n",
    "\n",
    "Create, in one figure, superimposed density plots of the data _with_ and _without_ seeding.\n",
    "\n",
    "<span class=\"t\">Hint<span class=\"c\">:</span></span>\n",
    "<span class=\"h\">\n",
    "Select both columns separately, and create a density plot from both of them. Add a legend to keep both density plots apart.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO [HYP_1c] Density plots of data (1 point)\n",
    "\n",
    "# ===== =====> Replace this line by your code. <===== ===== #\n",
    "\n",
    "#// END_TODO [HYP_1c]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1.d\n",
    "\n",
    "Create two separate (juxtaposed) Q-Q plots for the data _with_ and _without_ seeding. Place the plots next to each other and do not forget to add titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO [HYP_1d] Q-Q plot for data with and without seeding (1 point)\n",
    "\n",
    "# ===== =====> Replace this line by your code. <===== ===== #\n",
    "\n",
    "#// END_TODO [HYP_1d]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1.e\n",
    "\n",
    "Apply the Anderson-Darling normality test to the data _with_ and _without_ seeding, and assign the resulting **p-values** to the variables `p_anderson_darling_with` and `p_anderson_darling_without`, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO [HYP_1e] Anderson-Darling normality test for data with and without seeding (1 point)\n",
    "\n",
    "# ===== =====> Replace this line by your code. <===== ===== #\n",
    "\n",
    "#// END_TODO [HYP_1e]\n",
    "\n",
    "p_anderson_darling_with, p_anderson_darling_without"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above results you should notice the following things:\n",
    " * The long wobbly tails on the right in the kernel density plot indicate that this is not really a normal distribution. \n",
    " * In the Q-Q plots, the points stray far from the line.\n",
    " * In the Anderson-Darling test in both cases, the p-values are extremely small, indicating that these data sets are very unlikely under the assumption that they are from a normal distribution.\n",
    "\n",
    " This is a problem: the dataset is relatively small and the assumption of normality appears to be violated, which means that we should **not** apply a t-test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, a distribution skewed to the right can in some cases be transformed into in a normal distribution by taking the (natural) logarithm of the data.  \n",
    "To attempt this, we add two columns to the data frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clouds['with_log'] = np.log(df_clouds['with'])\n",
    "df_clouds['without_log'] = np.log(df_clouds['without'])\n",
    "df_clouds.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are now going to repeat the preceding normality tests on the transformed data to see if it now resembles a normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1.f\n",
    "Create density plots of the  log-transformed data\n",
    "\n",
    "<span class=\"t\">Hint<span class=\"c\">:</span></span>\n",
    "<span class=\"h\">\n",
    "Select both columns separately, and apply a density plot. Add a legend to keep both density plots apart.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO [HYP_1f] Density plots of log-transformed data (1 point)\n",
    "\n",
    "# ===== =====> Replace this line by your code. <===== ===== #\n",
    "\n",
    "#// END_TODO [HYP_1f]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1.g\n",
    "\n",
    "Create two separate (juxtaposed) Q-Q plots for the log-transformed data _with_ and _without_ seeding. Place the plots next to each other and do not forget to add titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO [HYP_1g] Q-Q plot for log-transformed data with and without seeding (1 point)\n",
    "\n",
    "# ===== =====> Replace this line by your code. <===== ===== #\n",
    "\n",
    "#// END_TODO [HYP_1g]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1.h\n",
    "Apply the Anderson-Darling normality test to the log-transformed data with and without cloud seeding. Assign the resulting **p-values** to `p_anderson_darling_with_log` and `p_anderson_darling_without_log`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO [HYP_1h] Anderson-Darling normality tests for log-transformed data (1 point)\n",
    "\n",
    "# ===== =====> Replace this line by your code. <===== ===== #\n",
    "\n",
    "#// END_TODO [HYP_1h]\n",
    "\n",
    "p_anderson_darling_with_log, p_anderson_darling_without_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1.i\n",
    "\n",
    "Does the log-scaled data seem to follow a normal distribution? Assign your (boolean) answers to the variables `normal_cloud_with` and `normal_cloud_without`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO [HYP_1i] Does the log-scaled data follow a normal distribution (1 point)\n",
    "\n",
    "# ===== =====> Replace this line by your code. <===== ===== #\n",
    "\n",
    "#// END_TODO [HYP_1i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 (Cloud Seeding): Apply Appropriate Test\n",
    "\n",
    "#### Exercise 1.j\n",
    "Preprocess each of the two data samples separately. \n",
    "Use the **log-transformed** data! Assign the results to `d_with_log` and `d_without_log`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO [HYP_1j]  Preprocess the log-transformed data (1 point)\n",
    "\n",
    "# ===== =====> Replace this line by your code. <===== ===== #\n",
    "\n",
    "#// END_TODO [HYP_1j]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1.k\n",
    "Create the test object using the preprocessed data.\n",
    "Assign this test object to `cm_cloud_log`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO [HYP_1k] Create test object (1 point)\n",
    "\n",
    "# ===== =====> Replace this line by your code. <===== ===== #\n",
    "\n",
    "#// END_TODO [HYP_1k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1.l\n",
    "\n",
    "Perform the appropriate test for the hypothesis you chose in Exercise 1.b. Assign the resulting **p-value** to `p_cloud_log`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO [HYP_1l] apply test on (1 point)\n",
    "\n",
    "# ===== =====> Replace this line by your code. <===== ===== #\n",
    "\n",
    "#// END_TODO [HYP_1l]\n",
    "\n",
    "p_cloud_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5 (Cloud Seeding): Decision\n",
    "\n",
    "#### Exercise 1.m\n",
    "\n",
    "What is your conclusion? Is there sufficient evidence to reject the null hypothesis (at significance level $\\alpha=0.05$) based on the log-transformed data? Assign your (boolean) answer to the variables `reject_null_cloud_log`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO [HYP_1m] Decision for confidence limits (1 point)\n",
    "\n",
    "# ===== =====> Replace this line by your code. <===== ===== #\n",
    "\n",
    "#// END_TODO [HYP_1m]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Hypothesis Testing on Proportions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Objectives of This Section\n",
    "\n",
    "After this section, you should\n",
    "\n",
    "* know how to do _one-sided_ and _two-sided_ tests for **equality of proportions**\n",
    "    of _one-sample_ and _two-sample_ data;\n",
    "* know how to draw conclusions about acceptance or rejection of the null hypothesis based on:\n",
    "  * p-value resulting from the z-test;\n",
    "  * confidence intervals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Exams\n",
    "\n",
    "We now explore hypothesis testing on proportions.\n",
    "For these tests we only need the number of observations and the number of successes.\n",
    "\n",
    "Let us look at a class of students following a course.\n",
    "The class has 100 students, and 88 pass their exam.\n",
    "The previous year there were 89 students and 70 passed the exam.\n",
    "The teacher has made large improvements to her course according to the feedback she has gotten from student reviews.\n",
    "The expectation is therefore that the passing rate has gone up.\n",
    "\n",
    "We set up the data in these variables:\n",
    "\n",
    "* `students` for the total number of students (this year);\n",
    "* `passes` for the number of students who passed the exam (this year);\n",
    "* `students_previous` for the total number of students in the previous year;\n",
    "* `passes_previous` for the number of students that passed in the previous year.\n",
    "\n",
    "From this data we can compute the passing rate for both years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "students = 100\n",
    "passes = 88\n",
    "\n",
    "rate = passes / students\n",
    "\n",
    "students_previous = 89\n",
    "passes_previous = 70\n",
    "\n",
    "rate_previous = passes_previous / students_previous\n",
    "\n",
    "rate, rate_previous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we will follow the four steps as explained above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 (Exams): Define Quantitative Question\n",
    "\n",
    "From the introduction above we can conclude that we want to investigate whether or not the passing rate has improved. We can define the following quantitative question.\n",
    "\n",
    "> Is the proportion of students that passed the exam this year significantly larger than the proportion of students that passed last year?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 (Exams): Formulate Hypothesis\n",
    "\n",
    "Again we should pay attention to the following: do we want to test for the equality of means or the equality of proportions?\n",
    "\n",
    "In this case we are interested in the **fraction** of the students that have failed. Hence we are interested in the **equality of proportions**\n",
    "\n",
    "As defined in step 1, we want to investigate whether or not the passing rate has improved. From the context we have a clear indication of a direction in which we expect the proportion to change. Therefore we should formulate a **one-sided** alternative hypothesis. \n",
    "\n",
    "In short, we will define a **one-sided** hypothesis for **equality of proportions** on **two-sample** data\n",
    "\n",
    "The corresponding hypotheses are\n",
    "\n",
    "$$H_0: p_{new} = p_{old}$$\n",
    "\n",
    "$$H_a: p_{new} > p_{old}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 (Exams): Check Assumptions\n",
    "\n",
    "In the case of testing for equality of proportion we do not assume our data to come from a normal distribution.\n",
    "Therefore we do not have to check for normality and can thus ignore this step.\n",
    "There are other assumptions when testing proportions and ways to check these assumptions, but they are outside the scope of this course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 (Exams): Apply Appropriate Test\n",
    "\n",
    "We defined a **one-sided** hypothesis for **equality of proportions** on **two-sample data**. Now we can apply the appropriate test. In our case this is the\n",
    "\n",
    "* **z-test for equality of proportions**.\n",
    "\n",
    "We can extract the z-test statistic and p-value using\n",
    "\n",
    "**`sm.stats.proportions_ztest(count, number_observations, value=..., alternative=...)`**\n",
    "\n",
    "The `proportions_ztest()` function returns a pair: the z-test statistic and the corresponding **p-value**.\n",
    "\n",
    "#### z-test for Equality of Proportions (One-sample)\n",
    "\n",
    "In case of a **one-sample** test, the `proportions_ztest()` function takes the following arguments:\n",
    "- **`count`**: the number of positive observations;\n",
    "- **`number_observations`**: the total number of observations;\n",
    "- **`value=`**: the reference proportion you want to compare with;\n",
    "- **`alternative=`**: one of `'two-sided'`, `'larger'`, or `'smaller'`.\n",
    "\n",
    "#### z-test for Equality of Proportions (Two-sample)\n",
    "\n",
    "In case of a **two-sample** test, the `proportions_ztest()` function takes the following arguments:\n",
    "    \n",
    "- **`count`**: a list `[num_new_positive_observations, num_old_positive_observations]` containing the number of positive observations in the new and old samples\n",
    "- **`number_observations`**: a list `[num_new_observations, num_old_observations]` containing the total number of observations in the new and old samples\n",
    "- **`alternative=`**: one of `'two-sided'`, `'larger'`, or `'smaller'`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wish to test whether the proportion of students that passed the last exam is larger than the proportion of students that passed in the past (i.e. a two-sample test).\n",
    "For convenience, we repeat our hypotheses:\n",
    "\n",
    "$$H_0: p_{new}=p_{old}$$\n",
    "\n",
    "$$H_a: p_{new}> p_{old}$$\n",
    "\n",
    "We can now compute the p-value for the z-test for equality of proportions in the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ztest_exam = sm.stats.proportions_ztest([passes, passes_previous], [students, students_previous], alternative='larger')\n",
    "ztest_exam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract the p-value\n",
    "\n",
    "The **p-value** is again the second element of the returned tuple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ztest_exam[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Extract the Confidence Interval (One-sample)\n",
    "\n",
    "We can also compute a 95%-confidence interval `CI_exam` for the proportion of passing students,\n",
    "using function **`sm.stats.proportion_confint(count, number_observations, alpha=...)`**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CI_exam = sm.stats.proportion_confint(passes, students, alpha=0.05)\n",
    "CI_exam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rate_previous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5 (Exams): Decision\n",
    "\n",
    "The **p-value** and the **confidence interval** both express the same information in a different form. \n",
    "\n",
    "#### p-value\n",
    "\n",
    "The **p-value** is the easiest to interpret. When it is lower than a chosen $\\alpha$ (typically $0.05$) we reject the null hypothesis.\n",
    "\n",
    "For the p-value calculated in step 4, we can see that in the case of $\\alpha=0.05$ the *null hypothesis is rejected*, because the p-value is below 0.05.\n",
    "\n",
    "#### Confidence Interval\n",
    "\n",
    "The **confidence interval** can be interpreted in the following way. The confidence interval expresses the values between which the *proportion* lies with 95% confidence. If the confidence interval contains the reference value we cannot reject the null hypothesis. Otherwise, we reject the null hypothesis.\n",
    "\n",
    "From the confidence interval calculated in step 4, we can see that the *null hypothesis is rejected*, because it does not contain the reference value `rate_previous`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise: Silicon Wafers\n",
    "\n",
    "A wafer is a slice of silicon from which chips are made\n",
    "for use as integrated circuits in devices like smart phones.\n",
    "An important quality characteristic in integrated-circuit manufacturing is\n",
    "the thickness of the wafer.\n",
    "As a routine, the thickness is measured in a gage, only giving two results: good or bad.\n",
    "\n",
    "It is known from previous years that $15\\%$ of the wafers are bad.\n",
    "A new batch arrives and a sample of $50$ wafers is taken from this new batch.\n",
    "The number of bad wafers in this sample equals $13$.\n",
    "We would like to know whether or not there is a difference in the fraction of bad wafers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2.a\n",
    "\n",
    "Compute the fraction of bad wafers in the new sample. Assign the number of wafers and number of bad wafers to the variables `num_wafers` and `num_bad_wafers`,\n",
    "and the old and the new proportion to the variables\n",
    "`prop_bad_wafers_old` and `prop_bad_wafers_new`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO [HYP_2a] Wafers, bad wafers, old and new proportion (1 point)\n",
    "\n",
    "# ===== =====> Replace this line by your code. <===== ===== #\n",
    "\n",
    "#// END_TODO [HYP_2a]\n",
    "\n",
    "num_wafers, num_bad_wafers, prop_bad_wafers_old, prop_bad_wafers_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 (Silicon Wafers): Define a Quantitative Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2.b\n",
    "\n",
    "Choose the most appropriate quantitative question for our example.\n",
    "Assign `'a'`, `'b'` or `'c'` to the variable `question_wafers`\n",
    "\n",
    "> a. Has the proportion of bad wafers in the new batch changed compared to previous years?<br>\n",
    "> b. Has the average of bad wafers in the new batch changed compared to previous years?<br>\n",
    "> c. Is the proportion of bad wafers in the new batch greater than previous years?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO [HYP_2b] Choose quantitative question (1 point)\n",
    "\n",
    "# ===== =====> Replace this line by your code. <===== ===== #\n",
    "\n",
    "#// END_TODO [HYP_2b]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 (Silicon Wafers): Formulate Hypothesis\n",
    "\n",
    "#### Exercise 2.c\n",
    "\n",
    "Formulate the question you chose above as statistical hypotheses. Encode them as strings in the following way:\n",
    "\n",
    "- For the null hypothesis use `'mu_old=mu_new'` or `'p_old=p_new'`, using `mu` to denote means and `p`to denote proportions.\n",
    "- For the alternative hypothesis use `!=`, `>` or `<`, e.g. `'mu_old<mu_new'`.\n",
    "\n",
    "Assign the null and alternative hypotheses encoded as strings to variables `H_0_wafers` and `H_a_wafers`, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO [HYP_2c] Formulate Hypothesis (1 point)\n",
    "\n",
    "# ===== =====> Replace this line by your code. <===== ===== #\n",
    "\n",
    "#// END_TODO [HYP_2c]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 (Silicon Wafers): Check Assumptions\n",
    "#### Exercise 2.d\n",
    "If there are assumptions you can check please do so.\n",
    "\n",
    "Assign **True** to the variable `check_assump_wafers` if assumptions need to be checked, **False** if not.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO [HYP_2d] Check Assumptions (1 point)\n",
    "\n",
    "# ===== =====> Replace this line by your code. <===== ===== #\n",
    "\n",
    "#// END_TODO [HYP_2d]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 (Silicon Wafers): Apply Correct Test\n",
    "\n",
    "#### Exercise 2.e\n",
    "\n",
    "Choose and apply the correct test. Assign the resulting p-value to `p_wafers`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO [HYP_2e] Apply test (1 point)\n",
    "\n",
    "# ===== =====> Replace this line by your code. <===== ===== #\n",
    "\n",
    "#// END_TODO [HYP_2e]\n",
    "\n",
    "p_wafers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2.f\n",
    "\n",
    "Is there sufficient evidence to reject the null hypothesis (at significance level $\\alpha=0.05$)? Assign your (boolean) answer to the variable `reject_null_wafers`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO [HYP_2f] Has the fraction of bad wafers changed? (1 point)\n",
    "\n",
    "# ===== =====> Replace this line by your code. <===== ===== #\n",
    "\n",
    "#// END_TODO [HYP_2f]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Performing Diagnostics on Regression Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Objectives of This Section\n",
    "\n",
    "After this section, you should\n",
    "\n",
    "* know how to inspect normalized residuals of a linear model;\n",
    "* know how to interpret normalized residual values to indicate wrong model assumption or suspected outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Boiling Point of Water\n",
    "\n",
    "In this exercise we use the data set `./datasets/water.csv`. This data set describes an experiment conducted by James David Forbes in 1844 in the Alps to use the boiling point of water to estimate the altitude based on the air pressure. The idea is that it is easier to measure the boiling point of water than the air pressure. The boiling point is available in the column `'bp'` (in degrees Celsius) and the pressure is available in the column `'pres'` (in inches of mercury).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_forbes= pd.read_csv(\"./datasets/water.csv\")\n",
    "df_forbes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we inspect our data using a scatter plot. In this way we can get an idea of what order polynomial we should fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df_forbes.plot(kind='scatter', x='bp', y='pres', color='b')\n",
    "ax.set_xlabel('Boiling point (in degrees Celcius)')\n",
    "ax.set_ylabel('Pressure (in inches of mercury)')\n",
    "ax.set_title(\"Pressure vs boiling point of water\", size=16, weight='bold');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scatter plot suggests that a first order polynomial might be a good fit. Let's fit a regression model and investigate these observations.\n",
    "\n",
    "We fit a linear regression model with the pressure as dependent variable and the boiling point as independent variable. \n",
    "\n",
    "Remember that a simple linear regression model tries to find the best fit for $a$ and $b$ in\n",
    "$$ \\text{dependent\\_var} = a * \\text{independent\\_var} + b $$\n",
    "\n",
    "For this we again use the **Ordinary Least Squares** (OLS) method from the SciKit Learn library, the same as in the DMM exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_forbes[['bp']]\n",
    "y = df_forbes['pres']\n",
    "linear_model = LinearRegression()\n",
    "linear_model.fit(X, y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note:** `X` is a `DataFrame` and `y` is a `Series`.\n",
    "\n",
    "We can visualize the regression model with the _Seaborn_ library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.regplot(x=df_forbes['bp'], y=df_forbes['pres'], line_kws={'color': 'orange'}, ci=None)\n",
    "ax.set_xlabel('Boiling point (in degrees Celcius)')\n",
    "ax.set_ylabel('Pressure (in inches of mercury)')\n",
    "ax.set_title(\"Pressure vs boiling point of water\", size=16, weight='bold');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Evaluation\n",
    "\n",
    "Let us check how well we fit our training data by computing the $R^2$ score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$R^2$ is almost 1. Does this mean we fitted the correct linear model?\n",
    "\n",
    "To evaluate the quality of our linear model, we plot the normalized residuals. Let us compute the residuals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = linear_model.predict(X)\n",
    "residuals = (y - y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and normalize them using the `scale()` method from the SciKit Learn library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import scale\n",
    "\n",
    "residuals_norm = scale(residuals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can plot the normalized residuals against the independent variable (i.e. the boiling point)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_forbes_vis = df_forbes.copy()\n",
    "df_forbes_vis['residuals'] = residuals_norm\n",
    "\n",
    "ax = df_forbes_vis.plot(kind='scatter', x='bp', y='residuals', color='b')\n",
    "plt.axhline()\n",
    "ax.set_xlabel('Boiling point (in degrees Celcius)')\n",
    "ax.set_ylabel('Normalized residuals')\n",
    "ax.set_title('Normalized residuals for pressure', fontsize=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two things we should note from the above plot. First, the residuals do not seem unstructured. There might be a parabolic shape. This is not completely clear from the plot but such uncertainty indicates that we should at least investigate other regression models.\n",
    "\n",
    "Secondly, if we assume this parabolic shape, there might be an outlier: the point at around 96.5. This is an interesting find, as it was not visible from the visual inspection of the data.\n",
    "\n",
    "The first thing we should do here is investigating if there is a better fitting model. Theory around the phenomenon you are studying is usually a good place to start. Theory tells us that there is a linear relation between the temperature and the _log_ of the pressure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first fit a linear model on the logarithm of the pressure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_log = np.log(df_forbes['pres'])\n",
    "linear_model_log = LinearRegression()\n",
    "linear_model_log.fit(X, y_log);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then draw a scatter plot of the normalized residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_log_pred = linear_model_log.predict(X)\n",
    "residual_log = scale(y_log - y_log_pred)\n",
    "\n",
    "df_forbes_log_vis = df_forbes.copy()\n",
    "df_forbes_log_vis['residuals'] = residual_log\n",
    "\n",
    "ax = df_forbes_log_vis.plot(kind='scatter', x='bp', y='residuals', color='b')\n",
    "plt.axhline()\n",
    "ax.set_xlabel('Boiling point')\n",
    "ax.set_ylabel('Normalized residuals')\n",
    "ax.set_title('Normalized residuals for logarithm of pressure', size=16, weight='bold');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These residuals seem less structured, which indicates that the new model fits the data better.\n",
    "\n",
    "As indicated in the lecture a normalized residual value higher than 2.5 might indicate an outlier (rule of thumb). Since the normalized residual for the point with pressure 96.4 (log pressure 4.57) is larger than 2.5, this point should be investigated further.\n",
    "\n",
    "Ideally we would check with the data collector if there is a reason to exclude this point. Perhaps equipment malfunctioned. Since we can no longer ask Dr. Forbes, the rules of scientific integrity dictate that we just have to accept this data point, as we have no good arguments for removing it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Timber"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value of a timber tree is determined by the volume of it, but this can only be determined after the tree is cut down and sawn up. We somehow would like to predict the volume of the tree from the girth (the circumference) of the tree, and its height, which are easy to measure when the tree is still standing. We will use the dataset in `./datasets/trees.csv` which contains a table with three columns:\n",
    "\n",
    "- **`girth`**: the girth of a tree at breast height, in inches (1 foot = 12 inches)\n",
    "- **`height`**: the height of a tree, in feet\n",
    "- **`volume`**: the volume of timber produced from the tree, in cubic feet\n",
    "\n",
    "We will fit a regression model with girth and height as independent variables. \n",
    "\n",
    "(This exercise was adapted from Bingham and Fry (2010))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_timber = pd.read_csv('./datasets/trees.csv', index_col=0)\n",
    "df_timber.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3.a\n",
    "\n",
    "Create a scatter plot of timber volume against tree height."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO [HYP_3a] Height vs volume (1 point)\n",
    "\n",
    "# ===== =====> Replace this line by your code. <===== ===== #\n",
    "\n",
    "#// END_TODO [HYP_3a]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3.b\n",
    "\n",
    "Create a scatter plot of timber volume against tree girth. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO [HYP_3b] Girth vs. volume (1 point)\n",
    "\n",
    "# ===== =====> Replace this line by your code. <===== ===== #\n",
    "\n",
    "#// END_TODO [HYP_3b]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3.c\n",
    "\n",
    "What order polynomial appears to be a good fit for a regression model with volume as the dependent variable, and height and girth as independent variables?\n",
    "\n",
    "> a. A first order polynomial.<br>\n",
    "> b. A second order polynomial.<br>\n",
    "> c. A third order polynomial.\n",
    "\n",
    "Assign your answer (`'a'`, `'b'`, or `'c'`) to `poly_order_timber`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO [HYP_3c] Polynomial order (1 point)\n",
    "\n",
    "# ===== =====> Replace this line by your code. <===== ===== #\n",
    "\n",
    "#// END_TODO [HYP_3c]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Order Polynomial Model\n",
    "\n",
    "Regardless of your answer to the previous question, we will now fit a first order polynomial model and plot the normalized residuals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3.d\n",
    "\n",
    "Construct the training set that will be used for fitting the first order polynomial model, with volume as the dependent variable and both girth and height as independent variables. Assign the data for the independent variables to the data frame `X_timber` and the data for the dependent variable to the series `y_timber`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO [HYP_3d] Define the training dataset (1 point)\n",
    "\n",
    "# ===== =====> Replace this line by your code. <===== ===== #\n",
    "\n",
    "#// END_TODO [HYP_3d]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3.e\n",
    "\n",
    "Fit the first order model, with volume as the dependent variable and both girth and height as independent variables.\n",
    "Assign the model to the variable `lm_timber`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO [HYP_3e] Fit the first order polynomial model (1 point)\n",
    "\n",
    "# ===== =====> Replace this line by your code. <===== ===== #\n",
    "\n",
    "#// END_TODO [HYP_3e]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3.f\n",
    "\n",
    "Compute the $R^2$ score of the `lm_timber` model on the training data. Assign the score to variable `r_timber`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO [HYP_3f] Compute R2 score (1 point)\n",
    "\n",
    "# ===== =====> Replace this line by your code. <===== ===== #\n",
    "\n",
    "#// END_TODO [HYP_3f]\n",
    "\n",
    "r_timber"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3.g\n",
    "\n",
    "Compute the normalized residuals for the training data. Assign them to the variable `residuals_timber`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO [HYP_3g] Model summary (1 point)\n",
    "\n",
    "# ===== =====> Replace this line by your code. <===== ===== #\n",
    "\n",
    "#// END_TODO [HYP_3g]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3.h\n",
    "\n",
    "Create a residual plot using the normalized residuals, with `'girth'` on the $x$-axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO [HYP_3h] Residual plot (1 point)\n",
    "\n",
    "# ===== =====> Replace this line by your code. <===== ===== #\n",
    "\n",
    "#// END_TODO [HYP_3h]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the residual plot, how well does this first order polynomial model perform?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Better Model\n",
    "\n",
    "To help define a better fitting model we should look at the following:\n",
    "\n",
    "The tree girth can be viewed as its circumference. The radius $r$ of the tree can then be computed by \n",
    "\n",
    "$$r = \\mathsf{girth} / 2\\pi$$\n",
    "\n",
    "The volume of a cylinder of radius $r$ and height $h$ is equal to\n",
    "\n",
    "$$\\pi * r^2 * h$$\n",
    "\n",
    "Assuming the tree trunk extends to the top of the tree, we can use this formula to estimate the volume of the tree trunk.\n",
    "\n",
    "#### Exercise 3.i\n",
    "\n",
    "Add a column called `'volume_est'` to the dataframe `df_timber` with a volume estimate (in cubic feet) for the tree trunk based on the above statements. Use `math.pi` as an approximation of $\\pi$.\n",
    "\n",
    "<span class=\"t\">Hint<span class=\"c\">:</span></span>\n",
    "<span class=\"h\">\n",
    "Remember that `df_timber['girth']` is in inches.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO [HYP_3i] Add volume estimates (1 point)\n",
    "\n",
    "# ===== =====> Replace this line by your code. <===== ===== #\n",
    "\n",
    "#// END_TODO [HYP_3i]\n",
    "\n",
    "df_timber.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3.j\n",
    "Fit a regression model with `volume` as the dependent and `volume_est` as the independent variable. Assign the data for the independent variables to data frame `X_timber_est` and the model to `lm_timber_est`.\n",
    "\n",
    "<span class=\"t\">Hint<span class=\"c\">:</span></span>\n",
    "<span class=\"h\">\n",
    "Reuse the dependent variable `y_timber`.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO [HYP_3j] Fit the better model (1 point)\n",
    "\n",
    "# ===== =====> Replace this line by your code. <===== ===== #\n",
    "\n",
    "#// END_TODO [HYP_3j]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3.k\n",
    "\n",
    "Compute the normalized residuals for the training data in the previous exercise. Assign them to the variable `residuals_timber_est`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO [HYP_3k] Residuals for the better model (1 point)\n",
    "\n",
    "# ===== =====> Replace this line by your code. <===== ===== #\n",
    "\n",
    "#// END_TODO [HYP_3k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3.l\n",
    "Plot the normalized residuals in `residuals_timber_est`, with `'volume_est'` on the x-axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO [HYP_3l] Normalized residuals vs volume estimate (1 point)\n",
    "\n",
    "# ===== =====> Replace this line by your code. <===== ===== #\n",
    "\n",
    "#// END_TODO [HYP_3l]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3.m\n",
    "\n",
    "Are there any potential outliers?\n",
    "Assign your (boolean) answer to `outliers`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO [HYP_3m] Potential outliers (1 point)\n",
    "\n",
    "# ===== =====> Replace this line by your code. <===== ===== #\n",
    "\n",
    "#// END_TODO [HYP_3m]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Exercise: Hypothesis Testing\n",
    "\n",
    "Last week we applied data aggregation and sampling to extract features from the mouse experiment data. Now we will use that same data to verify Fitts's law and draw conclusions about perceived differences in this data.\n",
    "\n",
    "Fitts's law describes the relation between the distance from a cursor to an object, the size of the object and the time it takes for a user to click the object. In the mouse experiment data was recorded using two different input devices: trackpad and mouse. We use this recorded data to compare the two input methods. \n",
    "\n",
    "One can imagine that the use of an interface feels intuitive as long as finding and clicking specific buttons does not take too long. More specifically, it does not matter how long it takes to click a button as long as it does not take longer than, say, one second. We would like to investigate if, for either of the input methods (trackpad vs. mouse), this limit is often exceeded.\n",
    "\n",
    "Let us first read the data into `df_fitts`, containing data from individual trials,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fitts = pd.read_csv(\"./datasets/user_trial_props.csv\")\n",
    "df_fitts.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and visualize the total time of a trial (in seconds) vs. the input method (0 for trackpad and 1 for mouse) using a strip plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.stripplot(x='input_method', y='total_time', data=df_fitts, jitter=True, s=1)\n",
    "ax.set_title(\"Strip plot of total time per input method\", size=16, weight='bold')\n",
    "ax.set_xlabel('Input method')\n",
    "ax.set_ylabel('Total time (s)')\n",
    "ax.set_ylim(0, 5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us extract the total time for the trials recorded using the mouse and trackpad into the series `mouse` and `trackpad`, respectively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mouse = df_fitts[df_fitts['input_method'] == 1]['total_time']\n",
    "trackpad = df_fitts[df_fitts['input_method'] == 0]['total_time']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4.a\n",
    "\n",
    "Using the goal described above, define a quantitative question, with which you define what you plan to investigate. Use this question to guide your thinking in the next exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign `'a'`, `'b'` or `'c'` to the variable `question_fitts`\n",
    "\n",
    ">a. Is the proportion of trackpad trajectories that take longer than one second significantly larger than the proportion of mouse trajectories that take longer than a second?<br>\n",
    "b. Is the proportion of trackpad trajectories that take longer than one second significantly different from the proportion of mouse trajectories that take longer than a second?<br>\n",
    "c. Is the proportion of trackpad trajectories that take longer than one second significantly smaller than the proportion of mouse trajectories that take longer than a second?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO [HYP_4a] Formulate quantitative question (1 point)\n",
    "\n",
    "# ===== =====> Replace this line by your code. <===== ===== #\n",
    "\n",
    "#// END_TODO [HYP_4a]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Exercise <span class=\"exercise\">4.b</span>  \n",
    "\n",
    "Using the goal described above and the quantitative question you defined, formulate the best fitting hypothesis. Think of the following:\n",
    "\n",
    "* one- or two-sample data;\n",
    "* one- or two-sided test;\n",
    "* equality of means or equality of proportions hypothesis.\n",
    "\n",
    "Assign to the variable `data_fitts` the string `'one-sample'` if we are dealing with one-sample data and `'two-sample'` if we are dealing with two-sample data.\n",
    "\n",
    "Assign to the variable `test_fitts` the string `'one-sided'` if a one-sided test should be used and `'two-sided'` if a two-sided test should be used.\n",
    "\n",
    "Assign the hypotheses as string to variables `H_0_fitts` and `H_a_fitts`. Construct the strings in the following way: `'mu_mouse=mu_trackpad'` or `'p_mouse=p_trackpad'`. Use `mu` when comparing means and `p` when comparing proportions. Use `!=`, `>` or `<` for the alternative hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO [HYP_4b] Formulate Hypothesis (1 point)\n",
    "\n",
    "# ===== =====> Replace this line by your code. <===== ===== #\n",
    "\n",
    "#// END_TODO [HYP_4b]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Exercise <span class=\"exercise\">4.c</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does the choice of hypothesis and the appropriate test for this hypothesis require you to test the normality assumptions? Assign boolean answer to `check_normal_fitts`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO [HYP_4c] Is there a need to check assumptions? (1 point)\n",
    "\n",
    "# ===== =====> Replace this line by your code. <===== ===== #\n",
    "\n",
    "#// END_TODO [HYP_4c]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Exercise <span class=\"exercise\">4.d</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you indicated the need to check normality assumptions in the previous question, please do so below. Assign a boolean answer to `normal_fitts`: \n",
    "\n",
    "- `True` if the assumptions are met;\n",
    "- `False` if there is enough evidence to reject normality, or the intended test does not require you to test for normality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO [HYP_4d] Check Assumptions (1 point)\n",
    "\n",
    "# ===== =====> Replace this line by your code. <===== ===== #\n",
    "\n",
    "#// END_TODO [HYP_4d]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Exercise <span class=\"exercise\" >4.e</span>\n",
    "Apply the correct test, regardless of the results you found in the previous question.\n",
    "It is possible that more preprocessing is required. Add more code cells between the markers if you need them. Assign the p-value you found to `p_fitts`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO [HYP_4e] Apply test (1 point)\n",
    "\n",
    "# ===== =====> Replace this line by your code. <===== ===== #\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// END_TODO [HYP_4e]\n",
    "\n",
    "p_fitts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Exercise <span class=\"exercise\">4.f</span> \n",
    "Is there sufficient evidence to reject the null hypothesis (at significance level $\\alpha=0.05$)? Assign your (boolean) answer to `reject_null_fitts`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO [HYP_4f] Decision (1 point)\n",
    "\n",
    "# ===== =====> Replace this line by your code. <===== ===== #\n",
    "\n",
    "#// END_TODO [HYP_4f]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Exercise: Regression Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitts's law describes the relation between the distance between a cursor and an object, the size of the object and the time it takes for a user to click the object. In particular, Fitts's law says that there is a linear relationship between trajectory's *total time* $T$ and its *Index of Difficulty* $I$:\n",
    "\n",
    "$$T = a * I + b$$\n",
    "\n",
    "where $I$ is given by\n",
    "\n",
    "$$I = \\log_2 \\left( \\frac{2D}{R} \\right)$$\n",
    "\n",
    "where $D$ is the *target distance* and $R$ is the *target radius*.\n",
    "\n",
    "In this exercise we will try to fit the Fitt's model to the trackpad data from the mouse experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Exercise 5.a\n",
    "\n",
    "Select the trackpad trajectories from `df_fitts`. Create a new dataframe `df_regression` with all the trackpad trajectories. Make a copy to avoid altering the original data frame.\n",
    "\n",
    "<span class=\"t\">Hint<span class=\"c\">:</span></span>\n",
    "<span class=\"h\">\n",
    "Trackpad trajectories have an `input_method` equal to 0.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO [HYP_5a] Create dataframe (1 point)\n",
    "\n",
    "# ===== =====> Replace this line by your code. <===== ===== #\n",
    "\n",
    "#// END_TODO [HYP_5a]\n",
    "\n",
    "df_regression.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To apply the Fitt's law we will need to construct additional features first. We start by computing the *target distance*, which we will then use together with the *target radius* to compute the *Index of Difficulty*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5.b\n",
    "\n",
    "Each trajectory starts at the origin, so with coordinates (0,0). Use this information to calculate the distance of the target from the starting point using the `'target_x'` and `'target_y'` columns. Store the target distance in a new column `'target_distance'` in `df_regression`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO [HYP_5b] Add distance column (1 point)\n",
    "\n",
    "# ===== =====> Replace this line by your code. <===== ===== #\n",
    "\n",
    "#// END_TODO [HYP_5b]\n",
    "\n",
    "df_regression.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5.c\n",
    "\n",
    "Recall that the *Index of Difficulty* $I$ is given by:\n",
    "\n",
    "$$I = \\log_2 \\left( \\frac{2 D}{R} \\right)$$\n",
    "\n",
    "where $D$ is the *target distance* and $R$ is the *target radius*.\n",
    "\n",
    "Add a column `'ID'` to the data frame containing the Index of Difficulty.\n",
    "\n",
    "<span class=\"t\">Hint<span class=\"c\">:</span></span>\n",
    "<span class=\"h\">\n",
    "Use the function `np.log2()`.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO [HYP_5c] Add ID column (1 point)\n",
    "\n",
    "# ===== =====> Replace this line by your code. <===== ===== #\n",
    "\n",
    "#// END_TODO [HYP_5c]\n",
    "\n",
    "df_regression.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5.d\n",
    "\n",
    "Plot a scatter plot of the index of difficulty `'total_time'` vs. `'ID'`. Restrict the $y$-axis from $0$ to $5$ seconds and use a point size of $5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO [HYP_5d] Create scatter diagram (1 point)\n",
    "\n",
    "# ===== =====> Replace this line by your code. <===== ===== #\n",
    "\n",
    "#// END_TODO [HYP_5d]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5.e\n",
    "\n",
    "Fit the appropriate regression model. Assign the data for the independent variables to the dataframe `X_fitts`, the data for the dependent variable to the series `y_fitts`, and the model to `lm_fitts`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO [HYP_5e] Fit model (1 point)\n",
    "\n",
    "# ===== =====> Replace this line by your code. <===== ===== #\n",
    "\n",
    "#// END_TODO [HYP_5e]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5.f\n",
    "\n",
    "Plot the normalized residuals, with `'ID'` on the x-axis. Assign the normalized residuals to the variable `residuals_fitts`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO [HYP_5f] Create scatter diagram of residuals (1 point)\n",
    "\n",
    "# ===== =====> Replace this line by your code. <===== ===== #\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// END_TODO [HYP_5f]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5.g\n",
    "\n",
    "Are there any data points suspected to be outliers, because of a high normalized residual? Assign your (boolean) answer to `outlier_fitts`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO [HYP_5g] Are there suspicious points? (1 point)\n",
    "\n",
    "# ===== =====> Replace this line by your code. <===== ===== #\n",
    "\n",
    "#// END_TODO [HYP_5g]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feedback\n",
    "\n",
    "Please fill in this questionaire to help us improve this course for the next year. Your feedback will be anonymized and will not affect your grade in any way!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many hours did you spend on these Exercises?\n",
    "\n",
    "Assign a number to `feedback_time`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_FEEDBACK [Feedback_1] (0 point)\n",
    "\n",
    "#// END_FEEDBACK [Feedback_1] (0 point)\n",
    "\n",
    "import numbers\n",
    "assert isinstance(feedback_time, numbers.Number), \"Please assign a number to feedback_time\"\n",
    "feedback_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How difficult did you find these Exercises?\n",
    "\n",
    "Assign an integer to `feedback_difficulty`, on a scale 0 - 10, with 0 being very easy, 5 being just right, and 10 being very difficult."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_FEEDBACK [Feedback_2] (0 point)\n",
    "\n",
    "#// END_FEEDBACK [Feedback_2] (0 point)\n",
    "\n",
    "import numbers\n",
    "assert isinstance(feedback_difficulty, numbers.Number), \"Please assign a number to feedback_difficulty\"\n",
    "feedback_difficulty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) What did you like?\n",
    "\n",
    "Assign a string to `feedback_like`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_FEEDBACK [Feedback_3] (0 point)\n",
    "\n",
    "#// END_FEEDBACK [Feedback_3] (0 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) What can be improved?\n",
    "\n",
    "Assign a string to `feedback_improve`. Please be specific, so that we can act on your feedback. For example, mention the specific exercises and what was unclear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_FEEDBACK [Feedback_4] (0 point)\n",
    "\n",
    "#// END_FEEDBACK [Feedback_4] (0 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "## How to Submit Your Work\n",
    "\n",
    "1. **Before submitting**, you must run your notebook by doing **Kernel > Restart & Run All**.  \n",
    "   Make sure that your notebook runs without errors **in linear order**.\n",
    "1. Remember to rename the notebook, replacing `...-template.ipynb` with `...-yourIDnr.ipynb`, where `yourIDnr` is your TU/e identification number.\n",
    "1. Submit the executed notebook with your work\n",
    "   for the appropriate assignment in **Canvas**.\n",
    "1. In the **Momotor** tab in Canvas,\n",
    "  you can select that assignment again to find some feedback on your submitted work.\n",
    "  If there are any problems reported by _Momotor_,\n",
    "  then you need to fix those,\n",
    "  and **resubmit the fixed notebook**.\n",
    "\n",
    "In case of a high workload on our server\n",
    "(because many students submit close to the deadline),\n",
    "it may take longer to receive the feedback.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all defined names\n",
    "%whos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# (End of Notebook) <span class=\"tocSkip\"></span>\n",
    "\n",
    "&copy; 2017-2023 - **TU/e** - Eindhoven University of Technology"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "268px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
